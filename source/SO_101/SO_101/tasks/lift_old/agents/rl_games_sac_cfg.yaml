params:
  seed: 42

  # The algo name must be changed to tell rl_games to load SACAgent
  algo:
    name: sac

  model:
    name: soft_actor_critic

  network:
    name: soft_actor_critic
    # [Key Checkpoint 1]: SAC must separate Actor and Critic
    # PPO can share encoder, but SAC in rl_games typically recommends separation
    separate: True

    space:
      continuous:
        # SAC typically doesn't need mu_activation specified here, it handles tanh squashing internally
        mu_activation: None
        sigma_activation: None
        mu_init:
          name: default
        sigma_init:
          name: const_initializer
          val: 0
        # [Key Checkpoint 2]: SAC must learn variance (log_std), absolutely cannot set fixed_sigma: True
        fixed_sigma: False

    mlp:
      units: [256, 128, 64]
      activation: elu
      d2rl: False
      initializer:
        name: default
      regularizer:
        name: None

    # [Key Checkpoint 3]: SAC-specific, bounds log_std to prevent gradient explosion
    log_std_bounds: [-5, 2]

  load_checkpoint: False
  load_path: ''

  config:
    name: franka_lift_sac
    env_name: rlgpu
    device: 'cuda:0'
    device_name: 'cuda:0'
    multi_gpu: False

    # --- Basic Settings ---
    normalize_input: True
    normalize_value: True
    reward_shaper:
      scale_value: 1.0  # SAC is sensitive to Reward Scale, recommend starting with 1.0

    # --- Training Loop (completely different from PPO) ---
    max_epochs: 6000
    save_best_after: 100
    save_frequency: 100
    print_stats: True

    # [Key Checkpoint 4]: PPO uses horizon_length, SAC uses num_steps_per_episode
    # Meaning: How many steps to collect into Buffer before starting one training update
    num_steps_per_episode: 8

    # --- Optimizer Parameters ---
    # SAC needs separate learning rates for Actor, Critic, and Alpha (entropy)
    actor_lr: 3e-4
    critic_lr: 3e-4
    alpha_lr: 3e-4

    # --- SAC Core Parameters ---
    # Automatic Entropy Tuning (Auto Entropy Tuning)
    learnable_temperature: True
    init_alpha: 1.0

    # Soft Update coefficient
    critic_tau: 0.005
    gamma: 0.99

    # --- Experience Replay Buffer ---
    # This is the hallmark of Off-Policy. If GPU memory is insufficient, change 1000000 to 100000
    replay_buffer_size: 1000000

    # How many samples to sample from Buffer for each update
    batch_size: 1024

    # --- Warmup ---
    # How many random steps to take before training starts to fill the Buffer
    num_warmup_steps: 1000

    # --- Gradient Related ---
    grad_norm: 1.0
    truncate_grads: True

    # --- PPO Parameters That Must Be Disabled ---
    # Intentionally left here commented out to remind you not to add them back, they will cause errors
    # horizon_length: ... (SAC doesn't use this)
    # mini_epochs: ... (SAC doesn't use this)
    # minibatch_size: ... (SAC uses batch_size)
    # e_clip: ... (This is PPO's Clip)

