======================================================================

[Train] Starting training with 2 parallel environments...
[ResidualRLPolicy] Initialized EMA smoother: alpha=0.3, joint_dim=5 (gripper excluded from smoothing)
[ResidualRLPolicy] Initialized history buffers: num_envs=2, history_len=10, action_dim=6, robot_state_dim=6
[ResidualRLPolicy] Initialized BASE action buffer: exec_horizon=10 (DiT predicts, PPO corrects each step)
[ResidualRLPolicy] === DiT predicts in chunks + PPO corrects step-by-step ===
[ResidualRLPolicy] obs shape: torch.Size([2, 32])
[ResidualRLPolicy] obs_normalized shape: torch.Size([2, 32])
[ResidualRLPolicy] Normalization enabled: norm_obs_mean=True, norm_action_mean=True
[ResidualRLPolicy] DEBUG - Raw obs first 6 values: [0.0, 0.0, 0.0, 0.0, 0.0, 0.4000000059604645]
[ResidualRLPolicy] DEBUG - Raw obs stats: min=-0.7069, max=1.0000, mean=0.0736
[ResidualRLPolicy] DEBUG - Checkpoint norm_obs_mean first 6: [0.0018288727151229978, 0.06071792170405388, 0.0025172559544444084, 1.2740063667297363, 1.3854997158050537, -0.18011225759983063]
[ResidualRLPolicy] DEBUG - Checkpoint norm_obs_std first 6: [0.555020272731781, 0.3330952227115631, 0.4607487618923187, 0.31861528754234314, 0.6615859270095825, 0.17746485769748688]
[ResidualRLPolicy] DEBUG - Normalized obs first 6 values: [-0.003295145696029067, -0.18228398263454437, -0.005463402718305588, -3.998572587966919, -2.094209671020508, 3.268885374069214]
[ResidualRLPolicy] DEBUG - base_action_t first 6: [0.14479373395442963, 0.026598496362566948, 0.03430429846048355, 1.3410688638687134, 1.4537110328674316, -0.09804268181324005]
[ResidualRLPolicy] robot_state: torch.Size([2, 6])
[ResidualRLPolicy] graph_embedding: torch.Size([2, 64])
[ResidualRLPolicy] base_action_t (denormalized): torch.Size([2, 6])
[ResidualRLPolicy] ppo_input (with base_action): torch.Size([2, 76])
[ResidualRLPolicy] residual_mean: torch.Size([2, 5])
[ResidualRLPolicy] DiT predicts every 10 steps, PPO corrects EVERY step

[DEBUG ActorCritic] Step 1:
  base_action_t[0]: [0.14479373395442963, 0.026598496362566948, 0.03430429846048355, 1.3410688638687134, 1.4537110328674316, -0.09804268181324005]
  residual[0]:      [-0.054106827825307846, -0.05956900119781494, 0.035200465470552444, 0.018469098955392838, -0.12735460698604584]
  scale:            0.0500
  final_action[0]:  [0.1420883983373642, 0.02362004667520523, 0.03606432303786278, 1.3419923782348633, 1.447343349456787, -0.09804268181324005]
  buffer_idx[0]:    1
[ResidualRLPolicy] Initialized EMA smoother: alpha=0.3, joint_dim=5 (gripper excluded from smoothing)
[ResidualRLPolicy] Initialized history buffers: num_envs=12, history_len=10, action_dim=6, robot_state_dim=6
[ResidualRLPolicy] Initialized BASE action buffer: exec_horizon=10 (DiT predicts, PPO corrects each step)
################################################################################
                        Learning iteration 0/10                         

                       Computation: 32 steps/s (collection: 1.196s, learning 0.278s)
             Mean action noise std: 0.00
                    value_function: 0.2154
                         surrogate: 0.3321
                           entropy: -4.4053
     Episode_Reward/lifting_object: 0.0000
    Episode_Reward/reaching_object: 0.0000
        Episode_Reward/action_rate: 0.0000
          Episode_Reward/joint_vel: 0.0000
            Curriculum/action_rate: -0.0001
              Curriculum/joint_vel: -0.0001
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0000
       Episode_Termination/success: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48
                    Iteration time: 1.47s
                      Time elapsed: 00:00:01
                               ETA: 00:00:14

Could not find git repository in /home/jacky_wang/miniconda3/envs/env_isaaclab/lib/python3.11/site-packages/rsl_rl/__init__.py. Skipping.
[ResidualRLPolicy] Initialized EMA smoother: alpha=0.3, joint_dim=5 (gripper excluded from smoothing)
[ResidualRLPolicy] Initialized history buffers: num_envs=2, history_len=10, action_dim=6, robot_state_dim=6
[ResidualRLPolicy] Initialized BASE action buffer: exec_horizon=10 (DiT predicts, PPO corrects each step)
[ResidualRLPolicy] Initialized EMA smoother: alpha=0.3, joint_dim=5 (gripper excluded from smoothing)
[ResidualRLPolicy] Initialized history buffers: num_envs=12, history_len=10, action_dim=6, robot_state_dim=6
[ResidualRLPolicy] Initialized BASE action buffer: exec_horizon=10 (DiT predicts, PPO corrects each step)
################################################################################
                        Learning iteration 1/10                         

                       Computation: 39 steps/s (collection: 1.024s, learning 0.204s)
             Mean action noise std: 0.00
                    value_function: 0.0081
                         surrogate: 0.3531
                           entropy: -4.3763
     Episode_Reward/lifting_object: 0.0000
    Episode_Reward/reaching_object: 0.0000
        Episode_Reward/action_rate: 0.0000
          Episode_Reward/joint_vel: 0.0000
            Curriculum/action_rate: -0.0001
              Curriculum/joint_vel: -0.0001
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0000
       Episode_Termination/success: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 96
                    Iteration time: 1.23s
                      Time elapsed: 00:00:02
                               ETA: 00:00:12

[ResidualRLPolicy] Initialized EMA smoother: alpha=0.3, joint_dim=5 (gripper excluded from smoothing)
[ResidualRLPolicy] Initialized history buffers: num_envs=2, history_len=10, action_dim=6, robot_state_dim=6
[ResidualRLPolicy] Initialized BASE action buffer: exec_horizon=10 (DiT predicts, PPO corrects each step)
[ResidualRLPolicy] Initialized EMA smoother: alpha=0.3, joint_dim=5 (gripper excluded from smoothing)
[ResidualRLPolicy] Initialized history buffers: num_envs=12, history_len=10, action_dim=6, robot_state_dim=6
[ResidualRLPolicy] Initialized BASE action buffer: exec_horizon=10 (DiT predicts, PPO corrects each step)
################################################################################
                        Learning iteration 2/10                         

                       Computation: 39 steps/s (collection: 1.008s, learning 0.198s)
             Mean action noise std: 0.00
                    value_function: 0.0012
                         surrogate: 0.2763
                           entropy: -4.3448
     Episode_Reward/lifting_object: 0.0000
    Episode_Reward/reaching_object: 0.0000
        Episode_Reward/action_rate: 0.0000
          Episode_Reward/joint_vel: 0.0000
            Curriculum/action_rate: -0.0001
              Curriculum/joint_vel: -0.0001
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0000
       Episode_Termination/success: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 144
                    Iteration time: 1.21s
                      Time elapsed: 00:00:03
                               ETA: 00:00:10

[ResidualRLPolicy] Initialized EMA smoother: alpha=0.3, joint_dim=5 (gripper excluded from smoothing)
[ResidualRLPolicy] Initialized history buffers: num_envs=2, history_len=10, action_dim=6, robot_state_dim=6
[ResidualRLPolicy] Initialized BASE action buffer: exec_horizon=10 (DiT predicts, PPO corrects each step)
[ResidualRLPolicy] Initialized EMA smoother: alpha=0.3, joint_dim=5 (gripper excluded from smoothing)
[ResidualRLPolicy] Initialized history buffers: num_envs=12, history_len=10, action_dim=6, robot_state_dim=6
[ResidualRLPolicy] Initialized BASE action buffer: exec_horizon=10 (DiT predicts, PPO corrects each step)
################################################################################
                        Learning iteration 3/10                         

                       Computation: 40 steps/s (collection: 0.991s, learning 0.199s)
             Mean action noise std: 0.00
                    value_function: 0.0009
                         surrogate: 0.2686
                           entropy: -4.3110
     Episode_Reward/lifting_object: 0.0000
    Episode_Reward/reaching_object: 0.0000
        Episode_Reward/action_rate: 0.0000
          Episode_Reward/joint_vel: 0.0000
            Curriculum/action_rate: -0.0001
              Curriculum/joint_vel: -0.0001
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.0000
       Episode_Termination/success: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 192
                    Iteration time: 1.19s
                      Time elapsed: 00:00:05
                               ETA: 00:00:08

[ResidualRLPolicy] Initialized EMA smoother: alpha=0.3, joint_dim=5 (gripper excluded from smoothing)
[ResidualRLPolicy] Initialized history buffers: num_envs=2, history_len=10, action_dim=6, robot_state_dim=6
[ResidualRLPolicy] Initialized BASE action buffer: exec_horizon=10 (DiT predicts, PPO corrects each step)
2026-01-14T18:07:38Z [19,373ms] [Warning] [carb] Client gpu.foundation.plugin has acquired [gpu::unstable::IMemoryBudgetManagerFactory v0.1] 100 times. Consider accessing this interface with carb::getCachedInterface() (Performance warning)
[ResidualRLPolicy] Initialized EMA smoother: alpha=0.3, joint_dim=5 (gripper excluded from smoothing)
[ResidualRLPolicy] Initialized history buffers: num_envs=12, history_len=10, action_dim=6, robot_state_dim=6
[ResidualRLPolicy] Initialized BASE action buffer: exec_horizon=10 (DiT predicts, PPO corrects each step)
################################################################################
                        Learning iteration 4/10                         

                       Computation: 33 steps/s (collection: 1.245s, learning 0.200s)
             Mean action noise std: 0.00
          Mean value_function loss: 0.0100
               Mean surrogate loss: 0.2502
                 Mean entropy loss: -4.2780
                       Mean reward: -0.00
               Mean episode length: 105.00
     Episode_Reward/lifting_object: 0.0000
    Episode_Reward/reaching_object: 0.0002
        Episode_Reward/action_rate: -0.0000
          Episode_Reward/joint_vel: -0.0003
            Curriculum/action_rate: -0.0001
              Curriculum/joint_vel: -0.0001
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.0000
       Episode_Termination/success: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 240
                    Iteration time: 1.44s
                      Time elapsed: 00:00:06
                               ETA: 00:00:07

[ResidualRLPolicy] Initialized EMA smoother: alpha=0.3, joint_dim=5 (gripper excluded from smoothing)
[ResidualRLPolicy] Initialized history buffers: num_envs=2, history_len=10, action_dim=6, robot_state_dim=6
[ResidualRLPolicy] Initialized BASE action buffer: exec_horizon=10 (DiT predicts, PPO corrects each step)
[ResidualRLPolicy] Initialized EMA smoother: alpha=0.3, joint_dim=5 (gripper excluded from smoothing)
[ResidualRLPolicy] Initialized history buffers: num_envs=12, history_len=10, action_dim=6, robot_state_dim=6
[ResidualRLPolicy] Initialized BASE action buffer: exec_horizon=10 (DiT predicts, PPO corrects each step)
################################################################################
                        Learning iteration 5/10                         

                       Computation: 39 steps/s (collection: 1.007s, learning 0.201s)
             Mean action noise std: 0.00
          Mean value_function loss: 0.0023
               Mean surrogate loss: 0.3911
                 Mean entropy loss: -4.2508
                       Mean reward: -0.00
               Mean episode length: 105.00
     Episode_Reward/lifting_object: 0.0000
    Episode_Reward/reaching_object: 0.0004
        Episode_Reward/action_rate: -0.0000
          Episode_Reward/joint_vel: -0.0005
            Curriculum/action_rate: -0.0001
              Curriculum/joint_vel: -0.0001
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.0000
       Episode_Termination/success: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 288
                    Iteration time: 1.21s
                      Time elapsed: 00:00:07
                               ETA: 00:00:06

[ResidualRLPolicy] Initialized EMA smoother: alpha=0.3, joint_dim=5 (gripper excluded from smoothing)
[ResidualRLPolicy] Initialized history buffers: num_envs=2, history_len=10, action_dim=6, robot_state_dim=6
[ResidualRLPolicy] Initialized BASE action buffer: exec_horizon=10 (DiT predicts, PPO corrects each step)
[ResidualRLPolicy] Initialized EMA smoother: alpha=0.3, joint_dim=5 (gripper excluded from smoothing)
[ResidualRLPolicy] Initialized history buffers: num_envs=12, history_len=10, action_dim=6, robot_state_dim=6
[ResidualRLPolicy] Initialized BASE action buffer: exec_horizon=10 (DiT predicts, PPO corrects each step)
################################################################################
                        Learning iteration 6/10                         

                       Computation: 39 steps/s (collection: 1.014s, learning 0.200s)
             Mean action noise std: 0.00
          Mean value_function loss: 0.0032
               Mean surrogate loss: 0.1776
                 Mean entropy loss: -4.2207
                       Mean reward: -0.00
               Mean episode length: 105.00
     Episode_Reward/lifting_object: 0.0000
    Episode_Reward/reaching_object: 0.0004
        Episode_Reward/action_rate: -0.0000
          Episode_Reward/joint_vel: -0.0005
            Curriculum/action_rate: -0.0001
              Curriculum/joint_vel: -0.0001
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.0000
       Episode_Termination/success: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 336
                    Iteration time: 1.21s
                      Time elapsed: 00:00:08
                               ETA: 00:00:05

[ResidualRLPolicy] Initialized EMA smoother: alpha=0.3, joint_dim=5 (gripper excluded from smoothing)
[ResidualRLPolicy] Initialized history buffers: num_envs=2, history_len=10, action_dim=6, robot_state_dim=6
[ResidualRLPolicy] Initialized BASE action buffer: exec_horizon=10 (DiT predicts, PPO corrects each step)
[ResidualRLPolicy] Initialized EMA smoother: alpha=0.3, joint_dim=5 (gripper excluded from smoothing)
[ResidualRLPolicy] Initialized history buffers: num_envs=12, history_len=10, action_dim=6, robot_state_dim=6
[ResidualRLPolicy] Initialized BASE action buffer: exec_horizon=10 (DiT predicts, PPO corrects each step)
