# Isaac Lab è‡ªå®šä¹‰ Policy è®­ç»ƒæŒ‡å—

## ğŸ“š æ¦‚è¿°

åœ¨ Isaac Lab ä¸­è‡ªå®šä¹‰ Policy æœ‰ä¸¤ç§æ–¹å¼ï¼š
1. **é…ç½®æ–¹å¼**ï¼šé€šè¿‡ä¿®æ”¹ `RslRlPpoActorCriticCfg` å‚æ•°ï¼ˆæ¨èï¼Œç®€å•ï¼‰
2. **å®ç°æ–¹å¼**ï¼šå®Œå…¨è‡ªå®šä¹‰ç½‘ç»œç»“æ„ï¼ˆé«˜çº§ï¼Œéœ€è¦æ·±å…¥äº†è§£ï¼‰

æœ¬æ–‡æ¡£ä¸»è¦ä»‹ç»**é…ç½®æ–¹å¼**ï¼Œè¿™æ˜¯æœ€å¸¸ç”¨çš„æ–¹æ³•ã€‚

---

## ğŸ—ï¸ Policy é…ç½®ç»“æ„

### 1. åŸºç¡€é…ç½®ç±»

```python
from isaaclab.utils import configclass
from isaaclab_rl.rsl_rl import (
    RslRlOnPolicyRunnerCfg,
    RslRlPpoActorCriticCfg,
    RslRlPpoAlgorithmCfg,
)

@configclass
class MyCustomPPORunnerCfg(RslRlOnPolicyRunnerCfg):
    # è®­ç»ƒå‚æ•°
    num_steps_per_env = 24        # æ¯ä¸ªç¯å¢ƒçš„æ­¥æ•°
    max_iterations = 1500         # æœ€å¤§è®­ç»ƒè¿­ä»£æ¬¡æ•°
    save_interval = 100           # ä¿å­˜é—´éš”
    experiment_name = "my_task"   # å®éªŒåç§°
    
    # Policy ç½‘ç»œç»“æ„é…ç½®
    policy = RslRlPpoActorCriticCfg(
        init_noise_std=1.0,                    # åˆå§‹æ¢ç´¢å™ªå£°æ ‡å‡†å·®
        actor_obs_normalization=True,          # Actor è§‚æµ‹å½’ä¸€åŒ–
        critic_obs_normalization=True,         # Critic è§‚æµ‹å½’ä¸€åŒ–
        actor_hidden_dims=[256, 128, 64],      # Actor ç½‘ç»œéšè—å±‚ç»´åº¦
        critic_hidden_dims=[256, 128, 64],     # Critic ç½‘ç»œéšè—å±‚ç»´åº¦
        activation="elu",                      # æ¿€æ´»å‡½æ•°: "elu", "relu", "tanh"
    )
    
    # PPO ç®—æ³•å‚æ•°
    algorithm = RslRlPpoAlgorithmCfg(
        value_loss_coef=1.0,           # ä»·å€¼æŸå¤±ç³»æ•°
        use_clipped_value_loss=True,    # ä½¿ç”¨è£å‰ªçš„ä»·å€¼æŸå¤±
        clip_param=0.2,                 # PPO è£å‰ªå‚æ•°
        entropy_coef=0.006,             # ç†µç³»æ•°ï¼ˆæ¢ç´¢å¥–åŠ±ï¼‰
        num_learning_epochs=5,          # æ¯æ¬¡æ›´æ–°çš„å­¦ä¹ è½®æ•°
        num_mini_batches=4,             # å°æ‰¹é‡æ•°é‡
        learning_rate=1.0e-4,           # å­¦ä¹ ç‡
        schedule="adaptive",            # å­¦ä¹ ç‡è°ƒåº¦: "adaptive", "constant"
        gamma=0.98,                     # æŠ˜æ‰£å› å­
        lam=0.95,                       # GAE lambda
        desired_kl=0.01,                # æœŸæœ›çš„ KL æ•£åº¦
        max_grad_norm=1.0,              # æ¢¯åº¦è£å‰ª
    )
```

---

## ğŸ“ ä¸»è¦é…ç½®å‚æ•°è¯´æ˜

### Policy ç½‘ç»œç»“æ„ (`RslRlPpoActorCriticCfg`)

| å‚æ•° | è¯´æ˜ | æ¨èå€¼ |
|------|------|--------|
| `actor_hidden_dims` | Actor ç½‘ç»œéšè—å±‚ç»´åº¦åˆ—è¡¨ | `[256, 128, 64]` (å¤§ä»»åŠ¡)<br/>`[128, 64]` (å°ä»»åŠ¡) |
| `critic_hidden_dims` | Critic ç½‘ç»œéšè—å±‚ç»´åº¦åˆ—è¡¨ | é€šå¸¸ä¸ Actor ç›¸åŒ |
| `activation` | æ¿€æ´»å‡½æ•° | `"elu"` (æ¨è)<br/>`"relu"`, `"tanh"` |
| `init_noise_std` | åˆå§‹æ¢ç´¢å™ªå£° | `0.5-2.0` (æ ¹æ®ä»»åŠ¡è°ƒæ•´) |
| `actor_obs_normalization` | Actor è§‚æµ‹å½’ä¸€åŒ– | `True` (æ¨è) |
| `critic_obs_normalization` | Critic è§‚æµ‹å½’ä¸€åŒ– | `True` (æ¨è) |

### è®­ç»ƒå‚æ•° (`RslRlPpoAlgorithmCfg`)

| å‚æ•° | è¯´æ˜ | æ¨èå€¼ |
|------|------|--------|
| `learning_rate` | å­¦ä¹ ç‡ | `1e-4` (é»˜è®¤)<br/>`1e-3` (ç®€å•ä»»åŠ¡)<br/>`1e-5` (å¤æ‚ä»»åŠ¡) |
| `num_steps_per_env` | æ¯ä¸ªç¯å¢ƒçš„æ­¥æ•° | `16-32` |
| `num_learning_epochs` | æ¯æ¬¡æ›´æ–°çš„å­¦ä¹ è½®æ•° | `5-10` |
| `num_mini_batches` | å°æ‰¹é‡æ•°é‡ | `4-8` |
| `gamma` | æŠ˜æ‰£å› å­ | `0.98-0.99` |
| `clip_param` | PPO è£å‰ªå‚æ•° | `0.1-0.3` |
| `entropy_coef` | ç†µç³»æ•°ï¼ˆæ¢ç´¢ï¼‰ | `0.001-0.01` |

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ­¥éª¤ 1: åˆ›å»ºè‡ªå®šä¹‰ Policy é…ç½®æ–‡ä»¶

åœ¨ `source/SO_100/SO_100/tasks/pick_place/agents/` ç›®å½•ä¸‹åˆ›å»ºæˆ–ä¿®æ”¹é…ç½®æ–‡ä»¶ï¼š

```python
# source/SO_100/SO_100/tasks/pick_place/agents/my_custom_policy_cfg.py

from isaaclab.utils import configclass
from isaaclab_rl.rsl_rl import (
    RslRlOnPolicyRunnerCfg,
    RslRlPpoActorCriticCfg,
    RslRlPpoAlgorithmCfg,
)

@configclass
class MyCustomPickPlacePPOCfg(RslRlOnPolicyRunnerCfg):
    num_steps_per_env = 24
    max_iterations = 2000
    save_interval = 100
    experiment_name = "pick_place_custom"
    empirical_normalization = False
    
    # è‡ªå®šä¹‰ Policy ç½‘ç»œç»“æ„
    policy = RslRlPpoActorCriticCfg(
        init_noise_std=0.5,
        actor_obs_normalization=True,
        critic_obs_normalization=True,
        actor_hidden_dims=[512, 256, 128],      # æ›´å¤§çš„ç½‘ç»œ
        critic_hidden_dims=[512, 256, 128],
        activation="elu",
    )
    
    # è‡ªå®šä¹‰ç®—æ³•å‚æ•°
    algorithm = RslRlPpoAlgorithmCfg(
        value_loss_coef=1.0,
        use_clipped_value_loss=True,
        clip_param=0.2,
        entropy_coef=0.01,                      # æ›´å¤šæ¢ç´¢
        num_learning_epochs=8,
        num_mini_batches=8,
        learning_rate=5.0e-5,                   # è¾ƒå°çš„å­¦ä¹ ç‡
        schedule="adaptive",
        gamma=0.99,
        lam=0.95,
        desired_kl=0.01,
        max_grad_norm=1.0,
    )
```

### æ­¥éª¤ 2: æ³¨å†Œ Policy é…ç½®

åœ¨ç¯å¢ƒæ³¨å†Œæ—¶æ·»åŠ  `rsl_rl_cfg_entry_point`ï¼š

```python
# source/SO_100/SO_100/tasks/pick_place/__init__.py

from . import agents

gym.register(
    id="SO-ARM100-Pick-Place-DualArm-IK-Abs-v0",
    entry_point=_ENTRY_POINT_ABS,
    kwargs={
        "env_cfg_entry_point": _ENV_CFG_ABS,
        "rsl_rl_cfg_entry_point": f"{agents.__name__}.my_custom_policy_cfg:MyCustomPickPlacePPOCfg",
    },
    disable_env_checker=True,
)
```

### æ­¥éª¤ 3: å¼€å§‹è®­ç»ƒ

```bash
python scripts/rsl_rl/train.py \
    --task SO-ARM100-Pick-Place-DualArm-IK-Abs-v0 \
    --headless
```

---

## ğŸ¨ å¸¸è§è‡ªå®šä¹‰åœºæ™¯

### åœºæ™¯ 1: æ›´å¤§çš„ç½‘ç»œï¼ˆå¤æ‚ä»»åŠ¡ï¼‰

```python
policy = RslRlPpoActorCriticCfg(
    actor_hidden_dims=[512, 256, 128, 64],
    critic_hidden_dims=[512, 256, 128, 64],
    activation="elu",
)
```

### åœºæ™¯ 2: æ›´å°çš„ç½‘ç»œï¼ˆç®€å•ä»»åŠ¡ï¼Œå¿«é€Ÿè®­ç»ƒï¼‰

```python
policy = RslRlPpoActorCriticCfg(
    actor_hidden_dims=[128, 64],
    critic_hidden_dims=[128, 64],
    activation="relu",
)
```

### åœºæ™¯ 3: æ›´å¤šæ¢ç´¢

```python
algorithm = RslRlPpoAlgorithmCfg(
    entropy_coef=0.02,        # æ›´é«˜çš„ç†µç³»æ•°
    init_noise_std=1.5,       # æ›´å¤§çš„åˆå§‹å™ªå£°
    # ... å…¶ä»–å‚æ•°
)
```

### åœºæ™¯ 4: æ›´ç¨³å®šçš„è®­ç»ƒ

```python
algorithm = RslRlPpoAlgorithmCfg(
    learning_rate=1.0e-5,     # è¾ƒå°çš„å­¦ä¹ ç‡
    clip_param=0.15,          # æ›´å°çš„è£å‰ª
    num_learning_epochs=10,   # æ›´å¤šçš„å­¦ä¹ è½®æ•°
    # ... å…¶ä»–å‚æ•°
)
```

---

## ğŸ“Š æŸ¥çœ‹ç°æœ‰é…ç½®

å‚è€ƒç°æœ‰é…ç½®ï¼š
- `source/SO_100/SO_100/tasks/pick_place/agents/rsl_rl_ppo_cfg.py` - Pick-Place ä»»åŠ¡
- `source/SO_100/SO_100/tasks/lift/agents/rsl_rl_ppo_cfg.py` - Lift ä»»åŠ¡

---

## ğŸ”— å‚è€ƒèµ„æº

1. **Isaac Lab å®˜æ–¹æ–‡æ¡£**:
   - [è®­ç»ƒæŒ‡å—](https://docs.robotsfan.com/isaaclab/source/overview/reinforcement-learning/training_guide.html)
   - [ç¯å¢ƒè®¾è®¡æŒ‡å—](https://docs.robotsfan.com/isaaclab/source/setup/walkthrough/technical_env_design.html)

2. **RSL-RL æ–‡æ¡£**:
   - PPO ç®—æ³•è¯¦è§£
   - Actor-Critic ç½‘ç»œç»“æ„

3. **å®é™…ç¤ºä¾‹**:
   - æŸ¥çœ‹ `scripts/rsl_rl/train.py` äº†è§£è®­ç»ƒæµç¨‹
   - æŸ¥çœ‹ `scripts/rsl_rl/play.py` äº†è§£æ¨ç†æµç¨‹

---

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **ç½‘ç»œå¤§å°**: æ›´å¤§çš„ç½‘ç»œéœ€è¦æ›´å¤šè®­ç»ƒæ—¶é—´å’Œå†…å­˜
2. **å­¦ä¹ ç‡**: æ ¹æ®ä»»åŠ¡å¤æ‚åº¦è°ƒæ•´ï¼Œå¤æ‚ä»»åŠ¡ç”¨æ›´å°çš„å­¦ä¹ ç‡
3. **æ¢ç´¢**: ä½¿ç”¨ `entropy_coef` å’Œ `init_noise_std` æ§åˆ¶æ¢ç´¢ç¨‹åº¦
4. **è§‚æµ‹ç»´åº¦**: ç¡®ä¿ç½‘ç»œè¾“å…¥ç»´åº¦ä¸è§‚æµ‹ç©ºé—´åŒ¹é…ï¼ˆè‡ªåŠ¨å¤„ç†ï¼‰

---

## ğŸ¯ ä¸‹ä¸€æ­¥

1. åˆ›å»ºä½ çš„è‡ªå®šä¹‰ Policy é…ç½®æ–‡ä»¶
2. æ³¨å†Œåˆ°ç¯å¢ƒ entry point
3. å¼€å§‹è®­ç»ƒå¹¶ç›‘æ§æ€§èƒ½
4. æ ¹æ®è®­ç»ƒç»“æœè°ƒæ•´å‚æ•°

ç¥ä½ è®­ç»ƒé¡ºåˆ©ï¼ğŸš€


