# è‡ªå®šä¹‰ç­–ç•¥ï¼ˆPolicyï¼‰å®ç°å®Œæ•´æŒ‡å—

## ğŸ“š ä¸‰ç§æ–¹å¼å¯¹æ¯”

| æ–¹å¼ | éš¾åº¦ | çµæ´»æ€§ | é€‚ç”¨åœºæ™¯ |
|------|------|--------|----------|
| **1. é…ç½®è‡ªå®šä¹‰** | â­ ç®€å• | ä¸­ | ä¿®æ”¹ç½‘ç»œç»“æ„ã€è¶…å‚æ•° |
| **2. Robomimic** | â­â­ ä¸­ç­‰ | ä¸­é«˜ | æ¨¡ä»¿å­¦ä¹ ã€åºåˆ—å»ºæ¨¡ |
| **3. å®Œå…¨è‡ªå®šä¹‰** | â­â­â­ å¤æ‚ | æœ€é«˜ | å®Œå…¨è‡ªå®šä¹‰ç½‘ç»œå’Œè®­ç»ƒæµç¨‹ |

---

## ğŸ¯ æ–¹å¼ 1: é…ç½®è‡ªå®šä¹‰ï¼ˆæ¨èåˆå­¦è€…ï¼‰

### é€‚ç”¨åœºæ™¯
- ä¿®æ”¹ç½‘ç»œå±‚æ•°å’Œç»´åº¦
- è°ƒæ•´æ¿€æ´»å‡½æ•°
- ä¿®æ”¹è®­ç»ƒè¶…å‚æ•°
- ä½¿ç”¨ RSL-RL æ¡†æ¶

### å®ç°æ­¥éª¤

#### 1. åˆ›å»ºé…ç½®æ–‡ä»¶

**æ–‡ä»¶**: `source/SO_100/SO_100/tasks/pick_place/agents/my_custom_policy_cfg.py`

```python
from isaaclab.utils import configclass
from isaaclab_rl.rsl_rl import (
    RslRlOnPolicyRunnerCfg,
    RslRlPpoActorCriticCfg,
    RslRlPpoAlgorithmCfg,
)

@configclass
class MyCustomPickPlacePPOCfg(RslRlOnPolicyRunnerCfg):
    num_steps_per_env = 24
    max_iterations = 2000
    save_interval = 100
    experiment_name = "my_custom_policy"
    empirical_normalization = False
    
    # è‡ªå®šä¹‰ Policy ç½‘ç»œç»“æ„
    policy = RslRlPpoActorCriticCfg(
        init_noise_std=0.5,
        actor_obs_normalization=True,
        critic_obs_normalization=True,
        actor_hidden_dims=[512, 256, 128],      # è‡ªå®šä¹‰ Actor ç½‘ç»œ
        critic_hidden_dims=[512, 256, 128],     # è‡ªå®šä¹‰ Critic ç½‘ç»œ
        activation="elu",                       # æ¿€æ´»å‡½æ•°: "elu", "relu", "tanh"
    )
    
    # è‡ªå®šä¹‰ç®—æ³•å‚æ•°
    algorithm = RslRlPpoAlgorithmCfg(
        value_loss_coef=1.0,
        use_clipped_value_loss=True,
        clip_param=0.2,
        entropy_coef=0.01,
        num_learning_epochs=8,
        num_mini_batches=8,
        learning_rate=5.0e-5,
        schedule="adaptive",
        gamma=0.99,
        lam=0.95,
        desired_kl=0.01,
        max_grad_norm=1.0,
    )
```

#### 2. æ³¨å†Œé…ç½®

**æ–‡ä»¶**: `source/SO_100/SO_100/tasks/pick_place/__init__.py`

```python
from . import agents

gym.register(
    id="SO-ARM100-Pick-Place-DualArm-IK-Abs-v0",
    entry_point=_ENTRY_POINT_ABS,
    kwargs={
        "env_cfg_entry_point": _ENV_CFG_ABS,
        "rsl_rl_cfg_entry_point": f"{agents.__name__}.my_custom_policy_cfg:MyCustomPickPlacePPOCfg",
    },
    disable_env_checker=True,
)
```

#### 3. å¼€å§‹è®­ç»ƒ

```bash
python scripts/rsl_rl/train.py \
    --task SO-ARM100-Pick-Place-DualArm-IK-Abs-v0 \
    --headless
```

---

## ğŸ¯ æ–¹å¼ 2: Robomimicï¼ˆæ¨¡ä»¿å­¦ä¹ ï¼‰

### é€‚ç”¨åœºæ™¯
- ä½¿ç”¨æ¼”ç¤ºæ•°æ®è®­ç»ƒï¼ˆHDF5 æ•°æ®é›†ï¼‰
- éœ€è¦åºåˆ—å»ºæ¨¡ï¼ˆRNN/Transformerï¼‰
- æ¨¡ä»¿å­¦ä¹ ä»»åŠ¡

### å®ç°æ­¥éª¤

#### 1. åˆ›å»º Robomimic é…ç½®æ–‡ä»¶

**æ–‡ä»¶**: `source/SO_100/SO_100/tasks/pick_place/agents/robomimic/my_bc_policy.json`

```json
{
    "algo_name": "bc",
    "experiment": {
        "name": "my_bc_policy",
        "validate": false,
        "save": {
            "enabled": true,
            "every_n_epochs": 100,
            "on_best_rollout_success_rate": true
        }
    },
    "train": {
        "data": null,
        "batch_size": 100,
        "num_epochs": 2000,
        "seq_length": 10,
        "cuda": true
    },
    "algo": {
        "optim_params": {
            "policy": {
                "optimizer_type": "adam",
                "learning_rate": {
                    "initial": 0.001
                }
            }
        },
        "loss": {
            "l2_weight": 1.0
        },
        "actor_layer_dims": [512, 512, 256],
        "rnn": {
            "enabled": true,
            "horizon": 10,
            "hidden_dim": 400,
            "rnn_type": "LSTM",
            "num_layers": 2
        }
    },
    "observation": {
        "modalities": {
            "obs": {
                "low_dim": [
                    "actions",
                    "joint_pos",
                    "joint_vel",
                    "object",
                    "eef_pos",
                    "eef_quat",
                    "gripper_pos"
                ]
            }
        }
    }
}
```

#### 2. æ³¨å†Œé…ç½®

**æ–‡ä»¶**: `source/SO_100/SO_100/tasks/pick_place/__init__.py`

```python
from . import agents

gym.register(
    id="SO-ARM100-Pick-Place-DualArm-IK-Abs-v0",
    entry_point=_ENTRY_POINT_ABS,
    kwargs={
        "env_cfg_entry_point": _ENV_CFG_ABS,
        "robomimic_bc_cfg_entry_point": f"{agents.__name__}:robomimic/my_bc_policy.json",
    },
    disable_env_checker=True,
)
```

#### 3. è®­ç»ƒ

```bash
./isaaclab.sh -p scripts/imitation_learning/robomimic/train.py \
    --task SO-ARM100-Pick-Place-DualArm-IK-Abs-v0 \
    --algo bc \
    --dataset ./datasets/pick_place.hdf5 \
    --normalize_training_actions
```

---

## ğŸ¯ æ–¹å¼ 3: å®Œå…¨è‡ªå®šä¹‰ PyTorch è®­ç»ƒè„šæœ¬ï¼ˆæœ€é«˜çµæ´»æ€§ï¼‰

### é€‚ç”¨åœºæ™¯
- å®Œå…¨è‡ªå®šä¹‰ç½‘ç»œæ¶æ„ï¼ˆTransformerã€Diffusion ç­‰ï¼‰
- è‡ªå®šä¹‰è®­ç»ƒæµç¨‹
- ä¸ä¾èµ–ç°æœ‰æ¡†æ¶

### å®ç°æ­¥éª¤

#### 1. åˆ›å»ºè‡ªå®šä¹‰ Policy ç½‘ç»œ

**æ–‡ä»¶**: `source/SO_100/SO_100/policies/my_custom_policy.py`

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class MyCustomPolicy(nn.Module):
    """å®Œå…¨è‡ªå®šä¹‰çš„ Policy ç½‘ç»œã€‚
    
    è¾“å…¥: è§‚æµ‹ (obs_dim,)
    è¾“å‡º: åŠ¨ä½œ (action_dim,)
    """
    
    def __init__(self, obs_dim: int, action_dim: int, hidden_dims: list = [256, 256, 128]):
        super().__init__()
        
        # æ„å»ºç½‘ç»œå±‚
        layers = []
        input_dim = obs_dim
        
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(input_dim, hidden_dim))
            layers.append(nn.LayerNorm(hidden_dim))  # LayerNorm
            layers.append(nn.GELU())  # GELU æ¿€æ´»
            layers.append(nn.Dropout(0.1))  # Dropout
            input_dim = hidden_dim
        
        # è¾“å‡ºå±‚
        layers.append(nn.Linear(input_dim, action_dim))
        layers.append(nn.Tanh())  # è¾“å‡ºå½’ä¸€åŒ–åˆ° [-1, 1]
        
        self.network = nn.Sequential(*layers)
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.orthogonal_(m.weight, gain=0.01)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, obs: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­
        
        Args:
            obs: è§‚æµ‹å¼ é‡ [batch_size, obs_dim] æˆ– [obs_dim]
            
        Returns:
            actions: åŠ¨ä½œå¼ é‡ [batch_size, action_dim] æˆ– [action_dim]
        """
        return self.network(obs)
```

#### 2. åˆ›å»ºè®­ç»ƒè„šæœ¬

**æ–‡ä»¶**: `scripts/custom_policy/train_my_policy.py`

```python
"""å®Œå…¨è‡ªå®šä¹‰çš„ Policy è®­ç»ƒè„šæœ¬ã€‚

è¿™ä¸ªè„šæœ¬å±•ç¤ºäº†å¦‚ä½•ä»é›¶å¼€å§‹è®­ç»ƒä¸€ä¸ªè‡ªå®šä¹‰ Policyã€‚
"""

from isaaclab.app import AppLauncher

# Launch Isaac Sim
app_launcher = AppLauncher(headless=True)
simulation_app = app_launcher.app

"""Rest everything follows."""

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import h5py
import numpy as np
from tqdm import tqdm

import gymnasium as gym
import SO_100.tasks  # noqa: F401  # æ³¨å†Œç¯å¢ƒ
from SO_100.policies.my_custom_policy import MyCustomPolicy


class HDF5Dataset(Dataset):
    """HDF5 æ•°æ®é›†åŠ è½½å™¨"""
    
    def __init__(self, hdf5_path: str, obs_keys: list):
        self.hdf5_path = hdf5_path
        self.obs_keys = obs_keys
        self.episodes = []
        
        # åŠ è½½æ‰€æœ‰ episode
        with h5py.File(hdf5_path, 'r') as f:
            for demo_key in f['data'].keys():
                demo = f[f'data/{demo_key}']
                obs_dict = {key: np.array(demo['observations'][key]) for key in obs_keys}
                actions = np.array(demo['actions'])
                
                # å­˜å‚¨æ¯ä¸ª (obs, action) å¯¹
                for i in range(len(actions)):
                    obs = np.concatenate([obs_dict[key][i] for key in obs_keys])
                    self.episodes.append({
                        'obs': obs.astype(np.float32),
                        'action': actions[i].astype(np.float32)
                    })
    
    def __len__(self):
        return len(self.episodes)
    
    def __getitem__(self, idx):
        episode = self.episodes[idx]
        return torch.from_numpy(episode['obs']), torch.from_numpy(episode['action'])


def train_behavioral_cloning(
    task_name: str,
    dataset_path: str,
    obs_keys: list,
    obs_dim: int,
    action_dim: int,
    hidden_dims: list = [512, 256, 128],
    batch_size: int = 256,
    num_epochs: int = 100,
    learning_rate: float = 1e-3,
    device: str = "cuda",
    save_dir: str = "./logs/custom_policy",
):
    """è®­ç»ƒ Behavioral Cloning Policy
    
    Args:
        task_name: ç¯å¢ƒä»»åŠ¡åç§°
        dataset_path: HDF5 æ•°æ®é›†è·¯å¾„
        obs_keys: è§‚æµ‹é”®ååˆ—è¡¨
        obs_dim: è§‚æµ‹ç»´åº¦ï¼ˆæ‰€æœ‰è§‚æµ‹æ‹¼æ¥åçš„æ€»ç»´åº¦ï¼‰
        action_dim: åŠ¨ä½œç»´åº¦
        hidden_dims: éšè—å±‚ç»´åº¦åˆ—è¡¨
        batch_size: æ‰¹æ¬¡å¤§å°
        num_epochs: è®­ç»ƒè½®æ•°
        learning_rate: å­¦ä¹ ç‡
        device: è®¾å¤‡ ("cuda" æˆ– "cpu")
        save_dir: æ¨¡å‹ä¿å­˜ç›®å½•
    """
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(save_dir, exist_ok=True)
    
    # åˆ›å»ºç¯å¢ƒï¼ˆç”¨äºè·å–è§‚æµ‹å’ŒåŠ¨ä½œç»´åº¦ï¼‰
    env_cfg = None  # ä»ä»»åŠ¡ä¸­è‡ªåŠ¨è·å–
    env = gym.make(task_name, cfg=env_cfg)
    
    # è·å–å®é™…çš„è§‚æµ‹å’ŒåŠ¨ä½œç»´åº¦
    obs_space = env.observation_space
    action_space = env.action_space
    
    if hasattr(obs_space, 'shape'):
        actual_obs_dim = sum(obs_space.shape) if isinstance(obs_space.shape, tuple) else obs_space.shape[0]
    else:
        # å­—å…¸ç©ºé—´ï¼šéœ€è¦æ‰‹åŠ¨è®¡ç®—
        actual_obs_dim = obs_dim  # ä½¿ç”¨æä¾›çš„ç»´åº¦
    
    actual_action_dim = action_space.shape[0] if hasattr(action_space, 'shape') else action_dim
    
    print(f"[INFO] è§‚æµ‹ç»´åº¦: {actual_obs_dim}")
    print(f"[INFO] åŠ¨ä½œç»´åº¦: {actual_action_dim}")
    
    # åˆ›å»º Policy ç½‘ç»œ
    policy = MyCustomPolicy(
        obs_dim=actual_obs_dim,
        action_dim=actual_action_dim,
        hidden_dims=hidden_dims
    ).to(device)
    
    print(f"[INFO] Policy ç½‘ç»œç»“æ„:")
    print(policy)
    
    # åŠ è½½æ•°æ®é›†
    print(f"[INFO] åŠ è½½æ•°æ®é›†: {dataset_path}")
    dataset = HDF5Dataset(dataset_path, obs_keys)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)
    
    print(f"[INFO] æ•°æ®é›†å¤§å°: {len(dataset)}")
    
    # å®šä¹‰ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = optim.Adam(policy.parameters(), lr=learning_rate)
    criterion = nn.MSELoss()
    
    # è®­ç»ƒå¾ªç¯
    policy.train()
    for epoch in range(num_epochs):
        total_loss = 0.0
        num_batches = 0
        
        pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{num_epochs}")
        for obs, actions in pbar:
            obs = obs.to(device)
            actions = actions.to(device)
            
            # å‰å‘ä¼ æ’­
            pred_actions = policy(obs)
            
            # è®¡ç®—æŸå¤±
            loss = criterion(pred_actions, actions)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(policy.parameters(), max_norm=1.0)
            optimizer.step()
            
            total_loss += loss.item()
            num_batches += 1
            pbar.set_postfix({'loss': loss.item()})
        
        avg_loss = total_loss / num_batches
        print(f"[Epoch {epoch+1}/{num_epochs}] å¹³å‡æŸå¤±: {avg_loss:.6f}")
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % 10 == 0:
            checkpoint_path = os.path.join(save_dir, f"policy_epoch_{epoch+1}.pt")
            torch.save({
                'epoch': epoch,
                'policy_state_dict': policy.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': avg_loss,
            }, checkpoint_path)
            print(f"[INFO] ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_path}")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_path = os.path.join(save_dir, "policy_final.pt")
    torch.save(policy.state_dict(), final_path)
    print(f"[INFO] ä¿å­˜æœ€ç»ˆæ¨¡å‹: {final_path}")
    
    env.close()
    return policy


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="è®­ç»ƒè‡ªå®šä¹‰ Policy")
    parser.add_argument("--task", type=str, required=True, help="ä»»åŠ¡åç§°")
    parser.add_argument("--dataset", type=str, required=True, help="HDF5 æ•°æ®é›†è·¯å¾„")
    parser.add_argument("--obs_dim", type=int, default=72, help="è§‚æµ‹ç»´åº¦")
    parser.add_argument("--action_dim", type=int, default=8, help="åŠ¨ä½œç»´åº¦")
    parser.add_argument("--epochs", type=int, default=100, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, default=256, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--lr", type=float, default=1e-3, help="å­¦ä¹ ç‡")
    parser.add_argument("--save_dir", type=str, default="./logs/custom_policy", help="ä¿å­˜ç›®å½•")
    
    args = parser.parse_args()
    
    # è§‚æµ‹é”®åï¼ˆéœ€è¦ä¸æ•°æ®é›†åŒ¹é…ï¼‰
    obs_keys = [
        "actions",
        "joint_pos",
        "joint_vel",
        "object",
        "object_positions",
        "object_orientations",
        "eef_pos",
        "eef_quat",
        "gripper_pos"
    ]
    
    # å¼€å§‹è®­ç»ƒ
    train_behavioral_cloning(
        task_name=args.task,
        dataset_path=args.dataset,
        obs_keys=obs_keys,
        obs_dim=args.obs_dim,
        action_dim=args.action_dim,
        hidden_dims=[512, 256, 128],
        batch_size=args.batch_size,
        num_epochs=args.epochs,
        learning_rate=args.lr,
        save_dir=args.save_dir,
    )


if __name__ == "__main__":
    main()
    simulation_app.close()
```

#### 3. åˆ›å»ºæ¨ç†è„šæœ¬

**æ–‡ä»¶**: `scripts/custom_policy/play_my_policy.py`

```python
"""ä½¿ç”¨è®­ç»ƒå¥½çš„è‡ªå®šä¹‰ Policy è¿›è¡Œæ¨ç†"""

from isaaclab.app import AppLauncher

app_launcher = AppLauncher(headless=False)
simulation_app = app_launcher.app

"""Rest everything follows."""

import torch
import gymnasium as gym
import SO_100.tasks  # noqa: F401
from SO_100.policies.my_custom_policy import MyCustomPolicy


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="ä½¿ç”¨è‡ªå®šä¹‰ Policy æ¨ç†")
    parser.add_argument("--task", type=str, required=True, help="ä»»åŠ¡åç§°")
    parser.add_argument("--checkpoint", type=str, required=True, help="æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„")
    parser.add_argument("--obs_dim", type=int, default=72, help="è§‚æµ‹ç»´åº¦")
    parser.add_argument("--action_dim", type=int, default=8, help="åŠ¨ä½œç»´åº¦")
    
    args = parser.parse_args()
    
    # åˆ›å»ºç¯å¢ƒ
    env = gym.make(args.task)
    obs, _ = env.reset()
    
    # åŠ è½½ Policy
    policy = MyCustomPolicy(
        obs_dim=args.obs_dim,
        action_dim=args.action_dim,
        hidden_dims=[512, 256, 128]
    )
    
    checkpoint = torch.load(args.checkpoint, map_location='cpu')
    if isinstance(checkpoint, dict):
        policy.load_state_dict(checkpoint['policy_state_dict'])
    else:
        policy.load_state_dict(checkpoint)
    
    policy.eval()
    policy = policy.to(env.unwrapped.device)
    
    print(f"[INFO] åŠ è½½æ¨¡å‹: {args.checkpoint}")
    
    # æ¨ç†å¾ªç¯
    with torch.inference_mode():
        while simulation_app.is_running():
            # å¤„ç†è§‚æµ‹ï¼ˆå¦‚æœæ˜¯å­—å…¸ï¼Œéœ€è¦æ‹¼æ¥ï¼‰
            if isinstance(obs, dict):
                obs_tensor = torch.cat([torch.from_numpy(obs[key]).flatten() for key in obs.keys()], dim=0)
            else:
                obs_tensor = torch.from_numpy(obs).flatten()
            
            obs_tensor = obs_tensor.unsqueeze(0).to(env.unwrapped.device)
            
            # è·å–åŠ¨ä½œ
            action = policy(obs_tensor)
            action = action.cpu().numpy()[0]
            
            # æ‰§è¡ŒåŠ¨ä½œ
            obs, reward, terminated, truncated, info = env.step(action)
            
            if terminated or truncated:
                obs, _ = env.reset()
    
    env.close()


if __name__ == "__main__":
    main()
    simulation_app.close()
```

#### 4. è®­ç»ƒå’Œä½¿ç”¨

```bash
# è®­ç»ƒ
python scripts/custom_policy/train_my_policy.py \
    --task SO-ARM100-Pick-Place-DualArm-IK-Abs-v0 \
    --dataset ./datasets/pick_place.hdf5 \
    --obs_dim 72 \
    --action_dim 8 \
    --epochs 200 \
    --batch_size 256

# æ¨ç†
python scripts/custom_policy/play_my_policy.py \
    --task SO-ARM100-Pick-Place-DualArm-IK-Abs-v0 \
    --checkpoint ./logs/custom_policy/policy_final.pt \
    --obs_dim 72 \
    --action_dim 8
```

---

## ğŸ“Š ä¸‰ç§æ–¹å¼å¯¹æ¯”æ€»ç»“

### æ–¹å¼ 1: é…ç½®è‡ªå®šä¹‰
- âœ… æœ€ç®€å•
- âœ… ä½¿ç”¨ç°æœ‰æ¡†æ¶ï¼ˆRSL-RLï¼‰
- âœ… å¿«é€Ÿå¼€å§‹
- âŒ çµæ´»æ€§æœ‰é™

### æ–¹å¼ 2: Robomimic
- âœ… é€‚åˆæ¨¡ä»¿å­¦ä¹ 
- âœ… æ”¯æŒåºåˆ—å»ºæ¨¡
- âœ… é…ç½®çµæ´»
- âŒ éœ€è¦ HDF5 æ•°æ®é›†

### æ–¹å¼ 3: å®Œå…¨è‡ªå®šä¹‰
- âœ… å®Œå…¨æ§åˆ¶
- âœ… å¯ä»¥å®ç°ä»»ä½•ç½‘ç»œç»“æ„
- âœ… è‡ªå®šä¹‰è®­ç»ƒæµç¨‹
- âŒ éœ€è¦è‡ªå·±å®ç°è®­ç»ƒé€»è¾‘
- âŒ ä»£ç é‡è¾ƒå¤§

---

## ğŸ¯ æ¨èé€‰æ‹©

- **åˆå­¦è€…/å¿«é€ŸåŸå‹**: ä½¿ç”¨**æ–¹å¼ 1**ï¼ˆé…ç½®è‡ªå®šä¹‰ï¼‰
- **æ¨¡ä»¿å­¦ä¹ ä»»åŠ¡**: ä½¿ç”¨**æ–¹å¼ 2**ï¼ˆRobomimicï¼‰
- **é«˜çº§è‡ªå®šä¹‰éœ€æ±‚**: ä½¿ç”¨**æ–¹å¼ 3**ï¼ˆå®Œå…¨è‡ªå®šä¹‰ï¼‰

---

## ğŸ“ ä¸‹ä¸€æ­¥

1. é€‰æ‹©é€‚åˆä½ çš„æ–¹å¼
2. åˆ›å»ºç›¸åº”çš„é…ç½®æ–‡ä»¶/è„šæœ¬
3. å‡†å¤‡æ•°æ®é›†ï¼ˆå¦‚æœéœ€è¦ï¼‰
4. å¼€å§‹è®­ç»ƒ

ç¥ä½ è®­ç»ƒé¡ºåˆ©ï¼ğŸš€


